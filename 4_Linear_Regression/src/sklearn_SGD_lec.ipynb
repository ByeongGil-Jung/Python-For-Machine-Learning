{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Stochastic Gradient Descent '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Stochastic Gradient Descent \"\"\"\n",
    "# ** Stochastic Gradient Descent 마지막 부분에 4 가지 GD 방법에 대한 비교 및 차이가 잘 나와있다 !!\n",
    "\n",
    "# >> 실제로는 Gradient Descent 보다 SGD (Stochastic Gradient Descent) 를 더 많이 사용한다.\n",
    "# '확률적인 경사 하강법' 이라는 의미를 갖고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 기존의 GD 는 확장된 개념으로 Full-batch Gradient Descent 를 사용 '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 기존의 GD 는 확장된 개념으로 Full-batch Gradient Descent 를 사용 \"\"\"\n",
    "\n",
    "# 한 점이 아닌, 한번에 여러 개의 점의 gradient 를 업데이트 하는 방법이다.\n",
    "# >> 이를 Full-batch Gradient Descent 라고 한다.\n",
    "\n",
    "# < 장점 >\n",
    "# - GD 가 1 개의 데이터를 기준으로 미분하는 것과는 달리, 동시에 여러 데이터를 미분한다.\n",
    "# - 앞으로 일반적으로 GD = (full) batch GD 라고 가정한다.\n",
    "# - 모든 데이터 셋으로 학습\n",
    "# - 업데이트 감소 -> 계산상 속도의 효율성을 높임\n",
    "# - 안정적인 Cost 함수 수렴\n",
    "\n",
    "# < 단점 >\n",
    "# - 지역 최적화 가능 (전체 data 를 한번에 같이 넣기 때문)\n",
    "# - 메모리 문제 (-> 수십 억개의 데이터를 한번에 업데이트 하기엔 무리가 있음)\n",
    "# - 대규모 data set (-> model / parameter 업데이트가 느려짐)\n",
    "\n",
    "# => 즉, 메모리 문제 등으로 딥러닝, 초대규모 data set 에선 사용하기 힘들다.\n",
    "# => 이를 보완할 방법으로 SGD 사용 (특히 mini-batch SGD 는 제일 많이 사용함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' SGD 원리 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" SGD 원리 \"\"\"\n",
    "\n",
    "# - GD 처럼 차례대로 한 점씩 접근하는 것이 아닌, \n",
    "#   한 번에 랜덤한 여러 점에서 접근한다.\n",
    "\n",
    "# > 원래 용도는 data set 에서 random 하게 training sample 을 뽑은 후 학습할 때 사용한다.\n",
    "\n",
    "# < Pseudo Code >\n",
    "# - data (X) 를 넣기 전에 random shuffle.\n",
    "# - shuffle 한 X 를 한개 한개씩 불러와서 update\n",
    "#\n",
    "# procedure SGD:\n",
    "#     shuffle(X)\n",
    "#     for i in number of X:\n",
    "#         theta_j := (theta_j - a * (y^(i) - y(i)) * x_j(i))\n",
    "\n",
    "# < 장점 >\n",
    "# - 일부 문제에 대해 GD 보다 더 빨리 수렴\n",
    "# - 지역 최적화 회피\n",
    "\n",
    "# < 단점 >\n",
    "# - 빈번한 업데이트로 인해 전체적인 시간이 좀 오래 걸림\n",
    "# - 대용량 데이터 작업 시 시간이 오래 걸림\n",
    "# - 더 이상 cost 가 줄어들지 않는 시점의 발견이 어려움\n",
    "\n",
    "# => 이를 보완하기 위해 'Mini-batch (Stochastic) Gradient Descent' 개념이 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Mini-batch (Stochastic) Gradient Descent '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Mini-batch (Stochastic) Gradient Descent \"\"\"\n",
    "\n",
    "# 처리할 데이터의 영역을 분할하여 연산을 수행한다.\n",
    "\n",
    "# - 한 번에 일정량의 데이터를 랜덤하게 뽑아서 학습\n",
    "# - SGD 와 Batch GD 를 혼합한 기법\n",
    "# - 가장 일반적으로 많이 쓰이는 기법\n",
    "#   (딥러닝의 optimization 된 알고리즘의 기본이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Epoch & Batch-size '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Epoch & Batch-size \"\"\"\n",
    "\n",
    "# - Epoch ?\n",
    "#   :: 전체 데이터가 Training data 에 들어가는 횟수 (들어갈 때 카운팅)\n",
    "# - Full-batch 를 n 번 실행하면 n epoch\n",
    "# - Batch-size ?\n",
    "#   :: 한 번에 학습되는(update 되는) 데이터의 갯수\n",
    "\n",
    "# ex) 총 5,120 개의 Training data 에 512 batch-size 면\n",
    "#     몇 번 학습을 해야 1 epoch 가 되는가 ?\n",
    "# ans) 10 번\n",
    "\n",
    "# < Pseudo Code >\n",
    "# procedure Mini_Batch_SGD:\n",
    "#     shuffle(X)\n",
    "#     BS <- Batch Size\n",
    "#     // NB <- Number of Batches (== 1 epoch 이 되기 위해 도는 횟수)\n",
    "#     NB <- len(X)//BS\n",
    "#     for i in NB:\n",
    "#         theta_j := theta_j- a * sum(y^(k) - y(k), from: k = i*BS, to: (i+1)*BS) * x_j(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
